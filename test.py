
'''
Functions used to test the model
'''

from sklearn.metrics import log_loss
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import random
from tqdm import tqdm
import gc

# evalutation perplexity
def evaluate_perplexity(test_seq_in, test_seq_out, model, vocab_size):
    length = len(test_seq_in)
    entropy = 0
    # compute output in batches
    batch_size = 1000

    for i in tqdm(range(0, length, batch_size)):
        batch_x = test_seq_in[i:i+batch_size]
        batch_y = test_seq_out[i:i+batch_size]
        batch_y = to_categorical(batch_y, vocab_size)
        #pred_y = model.predict(batch_x)

        #entropy += log_loss(batch_y, pred_y, normalize=False)
        entropy += model.evaluate(batch_x, batch_y)[0] * batch_size
        
        del batch_x, batch_y
        gc.collect()
        
    if i < length:
        batch_x = test_seq_in[i:]
        batch_y = test_seq_out[i:]
        batch_y = to_categorical(batch_y, vocab_size)
        #pred_y = model.predict(batch_x)
        #entropy += log_loss(batch_y, pred_y, normalize=False)
        entropy += model.evaluate(batch_x, batch_y)[0] * (length-i)

        del batch_x, batch_y
        gc.collect()
    
    entropy = entropy/length
    
    return 2**entropy


def generate_text(model, tokenizer, seq_length, seed_text, n_words):
    result = list()
    in_text = seed_text
    print("seed text = ", in_text)
    #print(tokenizer.texts_to_sequences([in_text])[0])
    index_word = {v: k for k, v in tokenizer.word_index.items()}
    for _ in range(n_words):
        # encode text with tokenizer
        encoded = tokenizer.texts_to_sequences([in_text])[0]
        # truncate sequence to a fixed length
        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')
        #print(encoded)
        y = model.predict_classes(encoded, verbose=0)

        # map predicted word index to word
        out_word = index_word.get(y[0])

        # append to input
        in_text += " " + out_word
        #in_text.append(out_word)

        result.append(out_word)
    return " ".join(result)

def evaluate_model(model, tokenizer, seq_length, n_sample, test, test_seq_in, test_seq_out, n_words):
    #perplexity = evaluate_perplexity(test_seq_in, test_seq_out)
    #print("*******")
    #print("Perplexity of model = ", perplexity)

    for i in range(n_sample):
        sample = random.sample(test, 1)
        length = len(sample[0].split())
        
        start_range = len(sample[0].split()) - n_words
        if start_range > 0:
            start = random.sample(range(start_range),1)[0]
        else:
            start = 0
        print(start)

        print("********")
        print("Test Sample ", i, " :\n")

        seed_text = " ".join(sample[0].split()[start:(start+seq_length)])

        generated_text = generate_text(model, tokenizer, seq_length, seed_text, n_words=n_words)
        print()
        print("Actual text in transcript...")
        print(" ".join(sample[0].split()[(start+seq_length):(start+seq_length+n_words)]))
        print()
        print("Text generated by model...")
        print(generated_text)
        print()


