{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\V Chan\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run helper\n",
    "%run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from xml file\n",
    "tree = ET.parse('ted_en-20160408.xml')\n",
    "root = tree.getroot()\n",
    "# get all content\n",
    "all_transcript = [root[i][1].text for i in range(len(root))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_transcript = [clean1(transcript) for transcript in all_transcript]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to training, validation and testing\n",
    "train = clean_transcript[:1585]\n",
    "valid = clean_transcript[1585:1835]\n",
    "test = clean_transcript[1835:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_object(test, \"test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build word embedding with Keras's Tokenizer\n",
    "(later on can try Word2Vec or Glove for embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit vocab size by n_words\n",
    "#n_words = 60000 #almost all words\n",
    "n_words= 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign \"UNK\" as oov_token\n",
    "tokenizer = Tokenizer(oov_token=\"UNK\", num_words=n_words+1)\n",
    "# fit tokenizer on training text\n",
    "tokenizer.fit_on_texts(train)\n",
    "# modify word_index so that all vocabs not found is assigned \"UNK\" token\n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= n_words} # <= because tokenizer is 1 indexed\n",
    "tokenizer.word_index[tokenizer.oov_token] = n_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20002\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = unpickle_object(\"Models_GRU/tokenizer_20000.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Input/Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all transcript together to one corpus\n",
    "train_corpus = \" \".join(train)\n",
    "valid_corpus = \" \".join(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in each sequence\n",
    "# i.e. model will be learning from the first (n_seq-1) of words in the sequence to predict the last word\n",
    "n_seq = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161311.80000000002"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq) * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 3226236\n",
      "Total Sequences: 612418\n"
     ]
    }
   ],
   "source": [
    "train_seq = create_input_output(transcript=train_corpus, n_seq=n_seq)\n",
    "valid_seq = create_input_output(transcript=valid_corpus, n_seq=n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_object(train_seq, \"train_seq_50\")\n",
    "#pickle_object(valid_seq, \"valid_seq_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_set = set(train_seq)\n",
    "valid_seq_set = set(valid_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3225809, 612326)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq_set), len(valid_seq_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the train_seq\n",
    "train_seq_encode = tokenizer.texts_to_sequences(train_seq)\n",
    "# encode the valid_seq\n",
    "valid_seq_encode = tokenizer.texts_to_sequences(valid_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a subset of training and validation data to train smaller models for now\n",
    "#sub_size_train = int(len(train_seq_encode) * 1)\n",
    "#sub_size_valid = int(len(valid_seq_encode))\n",
    "#sub_size_train, sub_size_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((161310, 51), (22583, 51))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a subset of training and validation data to train smaller models for now\n",
    "#train_sub = np.array(random.sample(train_seq_encode, sub_size_train))\n",
    "#valid_sub = np.array(random.sample(valid_seq_encode, sub_size_valid))\n",
    "#train_sub.shape, valid_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sub = np.array(train_seq_encode[:sub_size_train])\n",
    "#valid_sub = np.array(valid_seq_encode[:sub_size_valid])\n",
    "train_sub = np.array(train_seq_encode)\n",
    "valid_sub = np.array(valid_seq_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "hidden_size = 256\n",
    "num_epoch = 100\n",
    "emb_size = 50\n",
    "num_layer = 2\n",
    "drop_out = 0.5\n",
    "seq_length = n_seq-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 50)            1000100   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 20, 256)           235776    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20002)             5140514   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20002)             0         \n",
      "=================================================================\n",
      "Total params: 6,770,374\n",
      "Trainable params: 6,770,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_GRU(vocab_size=vocab_size, seq_length=seq_length, emb_size=emb_size, \n",
    "                        num_layers=num_layer, drop_out=drop_out, hidde_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = \"LSTM.hdf5\"\n",
    "file_path = \"GRU_weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "#file_path = \"SimpleRNN_weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='loss',\n",
    "                            verbose=1, save_best_only=True, mode='min')\n",
    "earlystop = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
    "callback_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   89/12602 [..............................] - ETA: 2:16:42 - loss: 7.5626 - acc: 0.0395"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-2350a314014c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_sub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                     epochs=num_epoch, callbacks=callback_list)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2230\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2232\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(my_generator(train_sub, n_seq, vocab_size, tokenizer, batch_size),\n",
    "                   validation_data = my_generator(valid_sub, n_seq, vocab_size, tokenizer, batch_size),\n",
    "                    validation_steps = int(len(valid_sub)/10), steps_per_epoch = (len(train_sub)/batch_size),\n",
    "                    epochs=num_epoch, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            500100    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10002)             1010202   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10002)             0         \n",
      "=================================================================\n",
      "Total params: 1,651,102\n",
      "Trainable params: 1,651,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_LSTM(vocab_size=vocab_size, seq_length=seq_length, emb_size=emb_size, \n",
    "                        num_layers=num_layer, drop_out=drop_out, hidde_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"LSTM_weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='loss',\n",
    "                            verbose=1, save_best_only=True, mode='min')\n",
    "earlystop = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
    "callback_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "631/630 [==============================] - 321s 508ms/step - loss: 6.6525 - acc: 0.0437 - val_loss: 6.5001 - val_acc: 0.0458\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.65252, saving model to LSTM_weights.01-6.50.hdf5\n",
      "Epoch 2/100\n",
      "631/630 [==============================] - 317s 502ms/step - loss: 6.5420 - acc: 0.0439 - val_loss: 6.4947 - val_acc: 0.0475\n",
      "\n",
      "Epoch 00002: loss improved from 6.65252 to 6.54196, saving model to LSTM_weights.02-6.49.hdf5\n",
      "Epoch 3/100\n",
      "631/630 [==============================] - 313s 496ms/step - loss: 6.5310 - acc: 0.0440 - val_loss: 6.4755 - val_acc: 0.0449\n",
      "\n",
      "Epoch 00003: loss improved from 6.54196 to 6.53099, saving model to LSTM_weights.03-6.48.hdf5\n",
      "Epoch 4/100\n",
      "631/630 [==============================] - 315s 499ms/step - loss: 6.5353 - acc: 0.0441 - val_loss: 6.5016 - val_acc: 0.0463\n",
      "\n",
      "Epoch 00004: loss did not improve from 6.53099\n",
      "Epoch 5/100\n",
      "631/630 [==============================] - 321s 509ms/step - loss: 6.5279 - acc: 0.0438 - val_loss: 6.5212 - val_acc: 0.0329\n",
      "\n",
      "Epoch 00005: loss improved from 6.53099 to 6.52786, saving model to LSTM_weights.05-6.52.hdf5\n",
      "Epoch 6/100\n",
      "631/630 [==============================] - 328s 520ms/step - loss: 6.5249 - acc: 0.0458 - val_loss: 6.5160 - val_acc: 0.0457\n",
      "\n",
      "Epoch 00006: loss improved from 6.52786 to 6.52486, saving model to LSTM_weights.06-6.52.hdf5\n",
      "Epoch 7/100\n",
      "631/630 [==============================] - 322s 510ms/step - loss: 6.5301 - acc: 0.0452 - val_loss: 6.5247 - val_acc: 0.0350\n",
      "\n",
      "Epoch 00007: loss did not improve from 6.52486\n",
      "Epoch 8/100\n",
      "631/630 [==============================] - 317s 502ms/step - loss: 6.5359 - acc: 0.0442 - val_loss: 6.5056 - val_acc: 0.0462\n",
      "\n",
      "Epoch 00008: loss did not improve from 6.52486\n",
      "Epoch 9/100\n",
      "631/630 [==============================] - 318s 504ms/step - loss: 6.5410 - acc: 0.0434 - val_loss: 6.4874 - val_acc: 0.0468\n",
      "\n",
      "Epoch 00009: loss did not improve from 6.52486\n",
      "Epoch 10/100\n",
      "631/630 [==============================] - 318s 504ms/step - loss: 6.5342 - acc: 0.0449 - val_loss: 6.5028 - val_acc: 0.0467\n",
      "\n",
      "Epoch 00010: loss did not improve from 6.52486\n",
      "Epoch 11/100\n",
      "631/630 [==============================] - 319s 505ms/step - loss: 6.5334 - acc: 0.0449 - val_loss: 6.5248 - val_acc: 0.0465\n",
      "\n",
      "Epoch 00011: loss did not improve from 6.52486\n",
      "Epoch 12/100\n",
      "631/630 [==============================] - 317s 502ms/step - loss: 6.5303 - acc: 0.0446 - val_loss: 6.4804 - val_acc: 0.0459\n",
      "\n",
      "Epoch 00012: loss did not improve from 6.52486\n",
      "Epoch 13/100\n",
      "631/630 [==============================] - 320s 507ms/step - loss: 6.5298 - acc: 0.0448 - val_loss: 6.5109 - val_acc: 0.0448\n",
      "\n",
      "Epoch 00013: loss did not improve from 6.52486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f49df78a20>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(my_generator(train_sub, n_seq, vocab_size, tokenizer, batch_size),\n",
    "                   validation_data = my_generator(valid_sub, n_seq, vocab_size, tokenizer, batch_size),\n",
    "                    validation_steps = int(len(valid_sub)/batch_size), steps_per_epoch = (len(train_sub)/batch_size),\n",
    "                    epochs=num_epoch, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            500100    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 50, 100)           15100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10002)             1010202   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10002)             0         \n",
      "=================================================================\n",
      "Total params: 1,545,502\n",
      "Trainable params: 1,545,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_simpleRNN(vocab_size=vocab_size, seq_length=seq_length, emb_size=emb_size, \n",
    "                        num_layers=num_layer, drop_out=drop_out, hidde_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"SimpleRNN_weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='loss',\n",
    "                            verbose=1, save_best_only=True, mode='min')\n",
    "earlystop = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
    "callback_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "631/630 [==============================] - 329s 522ms/step - loss: 6.5202 - acc: 0.0448 - val_loss: 6.4998 - val_acc: 0.0476\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.52018, saving model to SimpleRNN_weights.01-6.50.hdf5\n",
      "Epoch 2/100\n",
      "631/630 [==============================] - 315s 499ms/step - loss: 6.5256 - acc: 0.0442 - val_loss: 6.5218 - val_acc: 0.0462\n",
      "\n",
      "Epoch 00002: loss did not improve from 6.52018\n",
      "Epoch 3/100\n",
      "631/630 [==============================] - 306s 485ms/step - loss: 6.5290 - acc: 0.0444 - val_loss: 6.4762 - val_acc: 0.0451\n",
      "\n",
      "Epoch 00003: loss did not improve from 6.52018\n",
      "Epoch 4/100\n",
      "631/630 [==============================] - 310s 491ms/step - loss: 6.5100 - acc: 0.0463 - val_loss: 6.5028 - val_acc: 0.0342\n",
      "\n",
      "Epoch 00004: loss improved from 6.52018 to 6.51000, saving model to SimpleRNN_weights.04-6.50.hdf5\n",
      "Epoch 5/100\n",
      "631/630 [==============================] - 305s 483ms/step - loss: 6.5116 - acc: 0.0445 - val_loss: 6.4718 - val_acc: 0.0459\n",
      "\n",
      "Epoch 00005: loss did not improve from 6.51000\n",
      "Epoch 6/100\n",
      "631/630 [==============================] - 317s 502ms/step - loss: 6.5324 - acc: 0.0437 - val_loss: 6.5312 - val_acc: 0.0440\n",
      "\n",
      "Epoch 00006: loss did not improve from 6.51000\n",
      "Epoch 7/100\n",
      "631/630 [==============================] - 321s 509ms/step - loss: 6.5145 - acc: 0.0438 - val_loss: 6.4979 - val_acc: 0.0481\n",
      "\n",
      "Epoch 00007: loss did not improve from 6.51000\n",
      "Epoch 8/100\n",
      "631/630 [==============================] - 314s 497ms/step - loss: 6.5054 - acc: 0.0449 - val_loss: 6.5506 - val_acc: 0.0455\n",
      "\n",
      "Epoch 00008: loss improved from 6.51000 to 6.50536, saving model to SimpleRNN_weights.08-6.55.hdf5\n",
      "Epoch 9/100\n",
      "631/630 [==============================] - 316s 500ms/step - loss: 6.5213 - acc: 0.0446 - val_loss: 6.4856 - val_acc: 0.0476\n",
      "\n",
      "Epoch 00009: loss did not improve from 6.50536\n",
      "Epoch 10/100\n",
      "631/630 [==============================] - 317s 503ms/step - loss: 6.5171 - acc: 0.0446 - val_loss: 6.5074 - val_acc: 0.0464\n",
      "\n",
      "Epoch 00010: loss did not improve from 6.50536\n",
      "Epoch 11/100\n",
      "631/630 [==============================] - 315s 500ms/step - loss: 6.5158 - acc: 0.0449 - val_loss: 6.4829 - val_acc: 0.0475\n",
      "\n",
      "Epoch 00011: loss did not improve from 6.50536\n",
      "Epoch 12/100\n",
      "631/630 [==============================] - 317s 503ms/step - loss: 6.5163 - acc: 0.0445 - val_loss: 6.4842 - val_acc: 0.0345\n",
      "\n",
      "Epoch 00012: loss did not improve from 6.50536\n",
      "Epoch 13/100\n",
      "631/630 [==============================] - 324s 513ms/step - loss: 6.5059 - acc: 0.0447 - val_loss: 6.5430 - val_acc: 0.0450\n",
      "\n",
      "Epoch 00013: loss did not improve from 6.50536\n",
      "Epoch 14/100\n",
      "631/630 [==============================] - 314s 498ms/step - loss: 6.5300 - acc: 0.0440 - val_loss: 6.5006 - val_acc: 0.0455\n",
      "\n",
      "Epoch 00014: loss did not improve from 6.50536\n",
      "Epoch 15/100\n",
      "631/630 [==============================] - 312s 495ms/step - loss: 6.5117 - acc: 0.0452 - val_loss: 6.5126 - val_acc: 0.0461\n",
      "\n",
      "Epoch 00015: loss did not improve from 6.50536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f4e057be48>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(my_generator(train_sub, n_seq, vocab_size, tokenizer, batch_size),\n",
    "                   validation_data = my_generator(valid_sub, n_seq, vocab_size, tokenizer, batch_size),\n",
    "                    validation_steps = int(len(valid_sub)/batch_size), steps_per_epoch = (len(train_sub)/batch_size),\n",
    "                    epochs=num_epoch, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.03514868, -0.04896992, -0.0353663 , ..., -0.01673633,\n",
       "          0.02039972, -0.02938935],\n",
       "        [ 0.03033654, -0.03940805,  0.0215332 , ...,  0.013695  ,\n",
       "         -0.00616841,  0.02095564],\n",
       "        [ 0.01305521,  0.0257215 , -0.05163691, ...,  0.04850791,\n",
       "         -0.00354804,  0.0213071 ],\n",
       "        ...,\n",
       "        [ 0.00067779, -0.046665  ,  0.02533393, ..., -0.00406514,\n",
       "         -0.04635901,  0.02779259],\n",
       "        [ 0.02223228,  0.01823927,  0.02873798, ...,  0.03120307,\n",
       "          0.01536973,  0.03065194],\n",
       "        [-0.00033187, -0.06757382, -0.06554747, ...,  0.05720245,\n",
       "          0.04770939, -0.02104107]], dtype=float32),\n",
       " array([[-0.03646696, -0.02038455,  0.02873883, ..., -0.00919431,\n",
       "         -0.08935077, -0.03084855],\n",
       "        [ 0.06633893,  0.03066442,  0.07749549, ...,  0.00984204,\n",
       "         -0.06950521, -0.08914451],\n",
       "        [ 0.01279981, -0.05528277, -0.0182763 , ..., -0.1357885 ,\n",
       "         -0.07286943, -0.04567435],\n",
       "        ...,\n",
       "        [ 0.08350104, -0.04029841, -0.04812527, ...,  0.02205041,\n",
       "          0.11617439,  0.02008849],\n",
       "        [ 0.00983305, -0.0297617 ,  0.00066807, ...,  0.05959875,\n",
       "         -0.0810767 ,  0.00972297],\n",
       "        [-0.02478793,  0.05888256, -0.06347974, ..., -0.07474257,\n",
       "         -0.11151571, -0.01254734]], dtype=float32),\n",
       " array([[ 0.04841318, -0.063339  ,  0.0542875 , ...,  0.03532121,\n",
       "          0.12030707,  0.00541851],\n",
       "        [-0.07941013, -0.02458049,  0.07983485, ..., -0.06862779,\n",
       "          0.0058224 ,  0.00942369],\n",
       "        [ 0.0055044 , -0.00613028,  0.0525293 , ...,  0.0141005 ,\n",
       "          0.00993267,  0.05428861],\n",
       "        ...,\n",
       "        [-0.06717856, -0.02448691, -0.09851383, ..., -0.04184714,\n",
       "         -0.08101732, -0.02550584],\n",
       "        [ 0.02090611,  0.0642356 , -0.02753319, ...,  0.01861903,\n",
       "          0.04117455,  0.10233033],\n",
       "        [-0.05602452,  0.00485933,  0.02234381, ...,  0.02599463,\n",
       "          0.0859351 , -0.05560041]], dtype=float32),\n",
       " array([ 0.02004403,  0.02378432,  0.01899172,  0.02595331,  0.02220182,\n",
       "         0.02636695,  0.02269047,  0.0232027 ,  0.02193723,  0.01903233,\n",
       "         0.02040377,  0.0219891 ,  0.02091183,  0.01584142,  0.02328592,\n",
       "         0.02201772,  0.01965771,  0.02246622,  0.02121342,  0.02026091,\n",
       "         0.02834337,  0.02186983,  0.02069208,  0.03137497,  0.02188802,\n",
       "         0.02496075,  0.02440129,  0.02182893,  0.02146177,  0.02207994,\n",
       "         0.01782656,  0.02431864,  0.02359719,  0.02494018,  0.02563066,\n",
       "         0.02320909,  0.01950095,  0.02343039,  0.01852585,  0.02194919,\n",
       "         0.01757776,  0.02047575,  0.02352244,  0.02220535,  0.02020052,\n",
       "         0.02065675,  0.01931328,  0.0230008 ,  0.02273563,  0.02562595,\n",
       "         0.02661939, -0.02348771,  0.02335108,  0.02433653,  0.02378433,\n",
       "         0.0247112 ,  0.02157156,  0.01904023,  0.02091523,  0.02240182,\n",
       "         0.02304317,  0.02311232,  0.02143677,  0.02031221,  0.02406463,\n",
       "         0.02803367,  0.02364448,  0.02645442,  0.0240301 ,  0.02244529,\n",
       "         0.02794402,  0.02296261,  0.017549  ,  0.02207759,  0.01825421,\n",
       "         0.02019859,  0.02550391,  0.02187489,  0.02308461,  0.02107942,\n",
       "         0.0230056 ,  0.02200158,  0.01834578,  0.02264075,  0.02495826,\n",
       "         0.02664082,  0.02531676,  0.02363049,  0.02348088,  0.02296785,\n",
       "         0.02089001,  0.01924482,  0.0214758 ,  0.01915432,  0.02495938,\n",
       "         0.02263458,  0.02185941,  0.02368973,  0.02515714,  0.02099025,\n",
       "         1.0219632 ,  1.0248307 ,  1.0193399 ,  1.0264912 ,  1.0222672 ,\n",
       "         1.0282503 ,  1.0226752 ,  1.0242928 ,  1.0240092 ,  1.0222088 ,\n",
       "         1.0214598 ,  1.022778  ,  1.0217367 ,  1.0165318 ,  1.0241956 ,\n",
       "         1.0220637 ,  1.0230467 ,  1.0238086 ,  1.0216936 ,  1.0201528 ,\n",
       "         1.0277644 ,  1.0236759 ,  1.0217739 ,  1.0326324 ,  1.0241342 ,\n",
       "         1.0261152 ,  1.0271549 ,  1.0236044 ,  1.021842  ,  1.0233344 ,\n",
       "         1.0174818 ,  1.0247052 ,  1.0252061 ,  1.0251046 ,  1.0277789 ,\n",
       "         1.0242028 ,  1.020113  ,  1.0241187 ,  1.019497  ,  1.0217566 ,\n",
       "         1.0178009 ,  1.0214627 ,  1.0269905 ,  1.0232553 ,  1.0215665 ,\n",
       "         1.0214369 ,  1.019953  ,  1.024095  ,  1.0243789 ,  1.0273604 ,\n",
       "         1.0286376 ,  0.98351955,  1.0248936 ,  1.02523   ,  1.024797  ,\n",
       "         1.0257417 ,  1.0218345 ,  1.0204517 ,  1.0220146 ,  1.0231009 ,\n",
       "         1.0234816 ,  1.0234793 ,  1.0221828 ,  1.0215249 ,  1.0248234 ,\n",
       "         1.0301864 ,  1.0241468 ,  1.0297548 ,  1.0249268 ,  1.0232716 ,\n",
       "         1.029605  ,  1.0236852 ,  1.0165578 ,  1.0237862 ,  1.0188473 ,\n",
       "         1.0209298 ,  1.0264171 ,  1.0227342 ,  1.0238267 ,  1.021085  ,\n",
       "         1.0240431 ,  1.0224049 ,  1.0176026 ,  1.0249048 ,  1.025967  ,\n",
       "         1.0286447 ,  1.0273339 ,  1.0247816 ,  1.0242596 ,  1.0240752 ,\n",
       "         1.0213443 ,  1.0198662 ,  1.0224164 ,  1.0198939 ,  1.0262529 ,\n",
       "         1.0235387 ,  1.021923  ,  1.0245767 ,  1.0259879 ,  1.0211409 ,\n",
       "         0.0262599 , -0.02522773,  0.02602836, -0.02591577, -0.02688502,\n",
       "         0.02724302, -0.02598899,  0.02601454, -0.02624153, -0.02639614,\n",
       "         0.01906244,  0.02255398, -0.0227294 , -0.01558381,  0.0261455 ,\n",
       "         0.02442193,  0.01311647, -0.02641442,  0.02559909, -0.02022207,\n",
       "         0.02629863,  0.02217069, -0.02624046,  0.02537727,  0.02370762,\n",
       "        -0.02696403,  0.02593143,  0.02227407, -0.02583183,  0.0237656 ,\n",
       "        -0.01783503,  0.01873311, -0.02498742,  0.02204193, -0.02419511,\n",
       "        -0.02593966,  0.01622202, -0.02469065,  0.02037057,  0.02636112,\n",
       "        -0.02953546, -0.02575906, -0.02610128, -0.02620289, -0.02477557,\n",
       "        -0.02553878, -0.02507156,  0.02203613,  0.02632624,  0.02509124,\n",
       "        -0.02200197, -0.02018541, -0.02584497,  0.02674425,  0.02597374,\n",
       "        -0.02552257,  0.02629914,  0.01953716, -0.02643123, -0.02622166,\n",
       "        -0.02399277, -0.02624984,  0.02461188,  0.0252769 , -0.02642716,\n",
       "        -0.02637279,  0.02653935, -0.02434925,  0.02648248, -0.01507709,\n",
       "        -0.02714295, -0.02644951, -0.01796654, -0.02639036, -0.01094302,\n",
       "        -0.0255299 ,  0.02653447,  0.01984414, -0.02542033, -0.02255928,\n",
       "         0.02601864, -0.02555828, -0.02166634,  0.02595151, -0.0259656 ,\n",
       "         0.02632542, -0.02288962, -0.0264894 , -0.02607276, -0.02576877,\n",
       "        -0.02554754, -0.01754341, -0.02191075, -0.02123026, -0.02621306,\n",
       "         0.02613876,  0.02326062, -0.02590895,  0.022131  ,  0.02643134,\n",
       "         0.02533271,  0.03518493,  0.03062326,  0.03423395,  0.0520917 ,\n",
       "         0.0404178 ,  0.03106364,  0.0309198 ,  0.02949529,  0.02898006,\n",
       "         0.03784861,  0.032686  ,  0.0306286 ,  0.02099402,  0.03013062,\n",
       "         0.03009487,  0.03052516,  0.02959892,  0.0271503 ,  0.02981457,\n",
       "         0.03628476,  0.0277371 ,  0.03759073,  0.03713335,  0.03344568,\n",
       "         0.03632513,  0.03511834,  0.02969926,  0.03000965,  0.03122828,\n",
       "         0.03081324,  0.03109544,  0.03437692,  0.0294357 ,  0.03763489,\n",
       "         0.02899197,  0.02374092,  0.02600603,  0.02462476,  0.0301675 ,\n",
       "         0.02389436,  0.03266647,  0.032736  ,  0.03311481,  0.02998159,\n",
       "         0.03005967,  0.03688569,  0.03460035,  0.03061507,  0.0439522 ,\n",
       "         0.0314095 , -0.02590124,  0.04251509,  0.03774649,  0.02855079,\n",
       "         0.03096174,  0.03177679,  0.02310678,  0.02719972,  0.03188386,\n",
       "         0.02809745,  0.03087122,  0.02777479,  0.03128695,  0.0290425 ,\n",
       "         0.03496659,  0.03597517, -0.00394093,  0.02885606,  0.03666636,\n",
       "         0.03739086,  0.02950524,  0.02277422,  0.03463709,  0.02710064,\n",
       "         0.02430131,  0.033012  ,  0.03214202,  0.03108656,  0.02703264,\n",
       "         0.02856611,  0.03023924,  0.02456713,  0.03326951,  0.03040173,\n",
       "         0.03289969,  0.03462209,  0.02841622,  0.02974312,  0.01315774,\n",
       "         0.03047232, -0.02989068,  0.03023935,  0.02904798,  0.03794629,\n",
       "         0.0300871 ,  0.03151258,  0.02715555,  0.03307234,  0.02958623],\n",
       "       dtype=float32),\n",
       " array([[-0.03330555,  0.00316184, -0.07470619, ...,  0.08754785,\n",
       "          0.08218876,  0.08219889],\n",
       "        [-0.11042952,  0.05808902,  0.05648959, ..., -0.03790851,\n",
       "          0.01308439, -0.06620485],\n",
       "        [-0.03768686,  0.00932286,  0.07063238, ..., -0.04007744,\n",
       "          0.03993967,  0.02543339],\n",
       "        ...,\n",
       "        [ 0.08379777,  0.04038275, -0.08927952, ..., -0.02475434,\n",
       "         -0.06248631, -0.07417018],\n",
       "        [ 0.07159465, -0.02146737, -0.08916441, ...,  0.11597005,\n",
       "         -0.06066285,  0.11339362],\n",
       "        [ 0.06082831,  0.05501677,  0.00624974, ..., -0.05551888,\n",
       "         -0.02320392, -0.03077609]], dtype=float32),\n",
       " array([[ 0.11250576,  0.01963215, -0.0212253 , ...,  0.02405952,\n",
       "          0.11127937, -0.01172916],\n",
       "        [-0.0623057 , -0.0605461 ,  0.00217326, ..., -0.01885363,\n",
       "         -0.10774481, -0.1536587 ],\n",
       "        [ 0.01339788,  0.10214683, -0.00631122, ...,  0.01787371,\n",
       "          0.03510555, -0.04865912],\n",
       "        ...,\n",
       "        [-0.03290721, -0.02313089,  0.00736947, ...,  0.04416364,\n",
       "         -0.00966054,  0.02488409],\n",
       "        [-0.07360464, -0.00335805,  0.0021951 , ..., -0.03778295,\n",
       "          0.04063417, -0.07266968],\n",
       "        [-0.03421762,  0.02968197, -0.01925567, ..., -0.03532343,\n",
       "         -0.07919175, -0.01704394]], dtype=float32),\n",
       " array([ 0.02065546,  0.02263866,  0.02133329,  0.02025773,  0.01947497,\n",
       "         0.02013387,  0.02174901,  0.02094027,  0.02145543,  0.01856014,\n",
       "         0.01995032, -0.04554373,  0.01999257,  0.02159387,  0.02594091,\n",
       "         0.01835883,  0.02013697,  0.02008161,  0.01418917,  0.01842571,\n",
       "         0.0196239 ,  0.02048873,  0.02092339,  0.02123127,  0.01669079,\n",
       "         0.02058477,  0.02126081,  0.0200657 ,  0.01922971,  0.02329927,\n",
       "         0.02398679,  0.0187622 ,  0.01847454,  0.02366678,  0.02431404,\n",
       "         0.01808374,  0.01965989,  0.01818325,  0.02167975,  0.01894197,\n",
       "         0.02007668,  0.02193699,  0.02095163,  0.01646769,  0.01781915,\n",
       "         0.02296155,  0.01752966,  0.02061653,  0.02223978,  0.01462022,\n",
       "         0.02213259,  0.01756779,  0.01828649,  0.021373  ,  0.01833414,\n",
       "         0.02103956,  0.02541726,  0.02151102,  0.01654787,  0.02021112,\n",
       "         0.01050477,  0.0204935 ,  0.02039471,  0.0212667 ,  0.0203212 ,\n",
       "         0.02097521,  0.01758779,  0.01966527,  0.01914465,  0.01950144,\n",
       "         0.02538099,  0.02645051,  0.0178494 ,  0.02014627,  0.02007454,\n",
       "         0.020271  ,  0.02076616,  0.01465575,  0.02154192,  0.02130058,\n",
       "         0.02204815,  0.01807345,  0.0192999 ,  0.01720755,  0.01703688,\n",
       "         0.02186833,  0.01760207,  0.02410067,  0.01993961,  0.02413211,\n",
       "         0.02153291,  0.01518734,  0.01172668,  0.01875943,  0.02085324,\n",
       "         0.0179162 ,  0.0229413 ,  0.01962777,  0.02111044,  0.0171827 ,\n",
       "         1.0227144 ,  1.0250508 ,  1.0218742 ,  1.0205934 ,  1.018856  ,\n",
       "         1.0213277 ,  1.0225953 ,  1.0216509 ,  1.022454  ,  1.0180687 ,\n",
       "         1.0210804 ,  0.96828216,  1.021551  ,  1.022023  ,  1.027127  ,\n",
       "         1.0192316 ,  1.0209651 ,  1.0208728 ,  1.0138656 ,  1.0191363 ,\n",
       "         1.0192481 ,  1.0214083 ,  1.0205564 ,  1.0216628 ,  1.0166587 ,\n",
       "         1.0212425 ,  1.0225619 ,  1.0209317 ,  1.0192933 ,  1.0245283 ,\n",
       "         1.0251113 ,  1.0197314 ,  1.0189501 ,  1.0246855 ,  1.0256025 ,\n",
       "         1.0177971 ,  1.0208927 ,  1.0194196 ,  1.0233687 ,  1.0193071 ,\n",
       "         1.0209321 ,  1.0232185 ,  1.0214696 ,  1.0181668 ,  1.0174531 ,\n",
       "         1.0238938 ,  1.0199411 ,  1.0225428 ,  1.0226622 ,  1.0148835 ,\n",
       "         1.0232826 ,  1.0188371 ,  1.0203013 ,  1.0219353 ,  1.0192081 ,\n",
       "         1.0223935 ,  1.0272131 ,  1.0218339 ,  1.0170952 ,  1.0209055 ,\n",
       "         1.011481  ,  1.0211862 ,  1.0205098 ,  1.020786  ,  1.020302  ,\n",
       "         1.0216502 ,  1.0179474 ,  1.020292  ,  1.0189904 ,  1.0185739 ,\n",
       "         1.026575  ,  1.0264841 ,  1.0181353 ,  1.0208899 ,  1.0208395 ,\n",
       "         1.0214047 ,  1.0215218 ,  1.0153657 ,  1.0214739 ,  1.0218146 ,\n",
       "         1.0228544 ,  1.0178883 ,  1.0189346 ,  1.0172261 ,  1.0175568 ,\n",
       "         1.0214376 ,  1.0186738 ,  1.026145  ,  1.0200282 ,  1.0253488 ,\n",
       "         1.0222057 ,  1.0161043 ,  1.0119865 ,  1.0189067 ,  1.0216211 ,\n",
       "         1.0192444 ,  1.0229808 ,  1.0207144 ,  1.0214628 ,  1.0185791 ,\n",
       "         0.02610011, -0.02015993,  0.02491604,  0.02519698, -0.02443097,\n",
       "         0.02419143,  0.02491629,  0.02352111,  0.02481417, -0.01811744,\n",
       "         0.01820812, -0.02716139,  0.01907842,  0.02526969,  0.02639467,\n",
       "        -0.02324905,  0.02445239,  0.02351108, -0.01543864,  0.02033614,\n",
       "         0.02306387,  0.02513655,  0.02639908, -0.02342867,  0.0196118 ,\n",
       "         0.02399873, -0.02303295, -0.02561059, -0.02419646,  0.02546284,\n",
       "         0.02706354, -0.01906046,  0.01818041,  0.02576138, -0.02505874,\n",
       "        -0.02111304, -0.01822149,  0.02385842,  0.02386927, -0.0196453 ,\n",
       "         0.02782707,  0.01703319, -0.02452533, -0.02098862,  0.01640886,\n",
       "        -0.02457669,  0.0220256 ,  0.02403978, -0.02534897, -0.00734   ,\n",
       "        -0.02449508,  0.02468323, -0.02607429,  0.02498583,  0.02328617,\n",
       "         0.02535631, -0.02564206,  0.02591588, -0.01021454,  0.01331288,\n",
       "        -0.00240602,  0.02449786, -0.02669189, -0.02446603,  0.02296583,\n",
       "         0.02416512, -0.01743883,  0.02447435, -0.0148573 ,  0.02155808,\n",
       "        -0.02578365, -0.02530844,  0.02073697, -0.02407146,  0.0248136 ,\n",
       "         0.02357577,  0.02414029,  0.00944437, -0.02560541, -0.02546257,\n",
       "         0.02504136,  0.00754675,  0.02003498,  0.02699845,  0.01934993,\n",
       "        -0.02017926, -0.02231978,  0.0254564 ,  0.0224299 ,  0.02461473,\n",
       "         0.02611218,  0.01906065,  0.00517067,  0.02127994, -0.02409623,\n",
       "        -0.01325489, -0.02742206, -0.0202521 , -0.02390896, -0.01725638,\n",
       "         0.03672274,  0.05441491,  0.03819671,  0.03416068,  0.03317314,\n",
       "         0.060671  ,  0.03460152,  0.03238453,  0.04206078,  0.05024195,\n",
       "         0.03605983,  0.02034391,  0.06114832,  0.03523197,  0.02697628,\n",
       "         0.03377393,  0.04591537,  0.05364523,  0.04149434,  0.06248884,\n",
       "         0.03628805,  0.03523832,  0.06223322,  0.0393394 ,  0.0409896 ,\n",
       "         0.03761626, -0.03612566,  0.05774774,  0.0306046 ,  0.06370132,\n",
       "         0.03419509,  0.06833849,  0.03432104,  0.04229883, -0.04363774,\n",
       "         0.03625203,  0.06562202,  0.06052143,  0.02924151,  0.04151465,\n",
       "        -0.06435278,  0.03559396,  0.0620044 ,  0.03510587, -0.04148472,\n",
       "         0.03351175,  0.04749573,  0.0291463 ,  0.04071909,  0.02657556,\n",
       "         0.03707753,  0.06584841,  0.03609728,  0.03290525,  0.03193551,\n",
       "         0.03336464,  0.06540681,  0.03949239,  0.05350024,  0.03941328,\n",
       "         0.02790516,  0.07634775,  0.03937679,  0.06205441,  0.05961809,\n",
       "         0.03435923,  0.02965252,  0.05314173,  0.02513603,  0.02925638,\n",
       "         0.06039962,  0.03854747,  0.03398474,  0.0644419 ,  0.03770173,\n",
       "         0.0455185 ,  0.04985679,  0.02200186, -0.05060948,  0.05981014,\n",
       "         0.03833624,  0.03281148,  0.03828633,  0.028132  ,  0.0298936 ,\n",
       "         0.05391842, -0.04473844,  0.03873495,  0.03295427,  0.03922997,\n",
       "         0.03188269,  0.03899916,  0.02941701,  0.04758325, -0.03360086,\n",
       "         0.02978576,  0.03558157,  0.04456742,  0.05058993,  0.04052847],\n",
       "       dtype=float32),\n",
       " array([[-0.25563526, -0.06796817, -0.0638243 , ..., -0.16759339,\n",
       "         -0.25569233, -0.06160446],\n",
       "        [ 0.25907972,  0.07427198,  0.04278508, ...,  0.23032361,\n",
       "          0.22933163,  0.05170022],\n",
       "        [-0.2799841 , -0.07111727, -0.04381477, ..., -0.12936625,\n",
       "         -0.06763005, -0.07816405],\n",
       "        ...,\n",
       "        [ 0.26335928,  0.06819506,  0.05177023, ...,  0.10663205,\n",
       "          0.17058925,  0.07195984],\n",
       "        [ 0.3018137 ,  0.06880752,  0.09024955, ...,  0.2978099 ,\n",
       "          0.18641004,  0.08515513],\n",
       "        [ 0.27183497,  0.05607494,  0.05583953, ...,  0.17333005,\n",
       "          0.428916  ,  0.07311881]], dtype=float32),\n",
       " array([-0.19655655,  1.3734248 ,  1.0635563 , ...,  0.04313078,\n",
       "        -0.08508958,  1.1964843 ], dtype=float32)]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text using Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 50)            1000100   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 512)               864768    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20002)             10261026  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20002)             0         \n",
      "=================================================================\n",
      "Total params: 12,125,894\n",
      "Trainable params: 12,125,894\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model = load_model('Models_GRU/GRU_weights.02-5.10.hdf5')\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 50)            1000100   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 20, 256)           235776    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20002)             5140514   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20002)             0         \n",
      "=================================================================\n",
      "Total params: 6,770,374\n",
      "Trainable params: 6,770,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model2 = load_model('Models_GRU/GRU_weights_2Layer.12-4.92.hdf5')\n",
    "test_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 50)            1000100   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 20, 256)           235776    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20002)             5140514   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20002)             0         \n",
      "=================================================================\n",
      "Total params: 6,770,374\n",
      "Trainable params: 6,770,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model3 = load_model('Models_GRU/GRU_2_weights.15-5.02.hdf5')\n",
    "test_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 50)            1000100   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 20, 256)           235776    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20002)             5140514   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20002)             0         \n",
      "=================================================================\n",
      "Total params: 6,770,374\n",
      "Trainable params: 6,770,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model4 = load_model('Models_GRU/GRU_2_weights.23-5.00.hdf5')\n",
    "test_model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You ve all seen lots of articles on climate change and here s yet another New York Times article just\n"
     ]
    }
   ],
   "source": [
    "sample = random.sample(test, 1)\n",
    "seed_text = \" \".join(sample[0].split()[:seq_length])\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You ve all seen lots of articles on climate change and here s yet another New York Times article just like every other darn one you ve seen It says all the same stuff as all the other ones you ve seen It even has the same amount of headline as all the other ones you ve seen What s unusual about this one maybe is that it s from\n"
     ]
    }
   ],
   "source": [
    "n_words = 50\n",
    "print(\" \".join(sample[0].split()[:seq_length+n_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU one layer, sequence length = 20\n",
      "\n",
      "You ve all seen lots of articles on climate change and here s yet another New York Times article just\n",
      "[11, 73, 34, 347, 528, 4, 4271, 23, 618, 161, 2, 72, 13, 316, 170, 108, 592, 253, 1893, 48]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a few years ago i was a baptist UNK and i was a teenager i was a teenager i was a teenager and i was a kid and i was a teenager i was a teenager i was a teenager i was a kid and i was a kid and'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"GRU one layer, sequence length = 20\\n\")\n",
    "generate_text(test_model, tokenizer, seq_length, seed_text, n_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU 2 layers (one epoch) sequence length= 20\n",
      "\n",
      "What I am always thinking about is what this session is about which is called simplicity And almost I would\n",
      "[22, 7, 285, 239, 249, 27, 12, 22, 14, 3767, 12, 27, 69, 12, 150, 4029, 2, 323, 7, 68]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'like to tell you a little bit about what i want to do with the world and i m going to tell you a little bit about what i m doing and i m going to tell you a little bit about what i m doing and i m going'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"GRU 2 layers (1st time, one epoch) sequence length= 20\\n\")\n",
    "generate_text(test_model2, tokenizer, seq_length, seed_text, n_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU 2 layers sequence length= 20\n",
      "\n",
      "What I am always thinking about is what this session is about which is called simplicity And almost I would\n",
      "[22, 7, 285, 239, 249, 27, 12, 22, 14, 3767, 12, 27, 69, 12, 150, 4029, 2, 323, 7, 68]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'like to show you a little bit of the UNK of the UNK of the UNK and the UNK of the UNK is a UNK of the UNK of the UNK and the UNK of the UNK of the UNK of the UNK and the UNK of the UNK of'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"GRU 2 layers (sequence length= 20\\n\")\n",
    "generate_text(test_model3, tokenizer, seq_length, seed_text, n_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU 2 layers sequence length= 20\n",
      "\n",
      "You ve all seen lots of articles on climate change and here s yet another New York Times article just\n",
      "[11, 73, 34, 347, 528, 4, 4271, 23, 618, 161, 2, 72, 13, 316, 170, 108, 592, 253, 1893, 48]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'in the last 20 years we were able to do the same thing and we had to do that we could be able to do that and we could be able to do that and we could be able to do that and we could be able to do that'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"GRU 2 layers sequence length= 20\\n\")\n",
    "generate_text(test_model4, tokenizer, seq_length, seed_text, n_words=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalutation perplexity\n",
    "def evaluate_perplexity(test_seq_in, test_seq_out, model):\n",
    "    length = len(test_seq_in)\n",
    "    entropy = 0\n",
    "    # compute output in batches\n",
    "    batch_size = 10000\n",
    "    \n",
    "    i = 0\n",
    "    if(length > batch_size):\n",
    "        while i < length:\n",
    "            batch_x = test_seq_in[i:i+batch_size]\n",
    "            batch_y = test_seq_out[i:i+batch_size]\n",
    "            pred_y = model.predict(batch_x)\n",
    "        \n",
    "            entropy += log_loss(batch_y, pred_y, normalize=False)\n",
    "            i += batch_size\n",
    "            print(\"calculating entropy... \",i)\n",
    "    \n",
    "    batch_x = test_seq_in[i:]\n",
    "    batch_y = test_seq_out[i:]\n",
    "    pred_y = model.predict(batch_x)\n",
    "        \n",
    "    entropy += log_loss(batch_y, pred_y[i:], normalize=False)\n",
    "    \n",
    "    entropy = entropy/length\n",
    "    \n",
    "    return 2**entropy       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = \" \".join(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 653772\n"
     ]
    }
   ],
   "source": [
    "test_seq = create_input_output(transcript=test_corpus, n_seq=n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_object(test_seq, \"test_seq_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the train_seq\n",
    "test_seq_encode = tokenizer.texts_to_sequences(test_seq)\n",
    "test_seq_in = np.array(test_seq_encode)[:,:-1]\n",
    "test_seq_out = np.array(test_seq_encode)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq_out = to_categorical(test_seq_out[:100], vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
